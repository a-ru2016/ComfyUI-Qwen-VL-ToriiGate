from __future__ import annotations
import torch
import os
import tempfile
import json
gc = __import__('gc')
import random
import re
import ast
from transformers import AutoModelForVision2Seq, AutoProcessor, AutoTokenizer, PhrasalConstraint
from huggingface_hub import snapshot_download
from modelscope.hub.snapshot_download import snapshot_download as modelscope_snapshot_download
from PIL import Image
from pathlib import Path
import folder_paths
from qwen_vl_utils import process_vision_info
import numpy as np
import requests
time = __import__('time')
import torchvision.io
from transformers import BitsAndBytesConfig
try:
    import cv2
    OPENCV_AVAILABLE = True
except ImportError:
    OPENCV_AVAILABLE = False
    print("Warning: OpenCV is not available, video processing may be limited.")

# æ¨¡å‹æ³¨å†Œè¡¨JSONæ–‡ä»¶è·¯å¾„
# Model registry JSON file path
MODEL_REGISTRY_JSON = os.path.join(os.path.dirname(os.path.abspath(__file__)), "model_registry.json")

def load_model_registry():
    """ä»JSONæ–‡ä»¶åŠ è½½æ¨¡å‹æ³¨å†Œè¡¨"""
    """Load the model registry from a JSON file"""
    try:
        with open(MODEL_REGISTRY_JSON, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"é”™è¯¯: æ¨¡å‹æ³¨å†Œè¡¨æ–‡ä»¶ {MODEL_REGISTRY_JSON} ä¸å­˜åœ¨" + " | " + f"Error: Model registry file {MODEL_REGISTRY_JSON} does not exist")
        return {}
    except json.JSONDecodeError as e:
        print(f"é”™è¯¯: è§£ææ¨¡å‹æ³¨å†Œè¡¨JSONæ–‡ä»¶æ—¶å‡ºé”™: {e}" + " | " + f"Error: Failed to parse model registry JSON file: {e}")
        return {}

# åŠ è½½æ¨¡å‹æ³¨å†Œè¡¨
# Load the model registry
MODEL_REGISTRY = load_model_registry()

def get_gpu_info():
    """è·å–GPUä¿¡æ¯ï¼ŒåŒ…æ‹¬æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    """Get GPU information, including memory usage"""
    try:
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            device = torch.cuda.current_device()
            props = torch.cuda.get_device_properties(device)
            total_memory = props.total_memory / 1024**3  # GB
            allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # GB
            free_memory = total_memory - allocated_memory
            
            return {
                "available": True,
                "count": gpu_count,
                "name": props.name,
                "total_memory": total_memory,
                "allocated_memory": allocated_memory,
                "free_memory": free_memory
            }
        else:
            return {
                "available": False,
                "count": 0,
                "name": "None",
                "total_memory": 0,
                "allocated_memory": 0,
                "free_memory": 0
            }
    except Exception as e:
        print(f"è·å–GPUä¿¡æ¯æ—¶å‡ºé”™: {e}" + " | " + f"Error getting GPU information: {e}")
        return {
            "available": False,
            "count": 0,
            "name": "None",
            "total_memory": 0,
            "allocated_memory": 0,
            "free_memory": 0
        }

def get_system_memory_info():
    """è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ€»å†…å­˜å’Œå¯ç”¨å†…å­˜"""
    """Get system memory information, including total and available memory"""
    try:
        import psutil
        mem = psutil.virtual_memory()
        return {
            "total": mem.total / 1024**3,  # GB
            "available": mem.available / 1024**3,  # GB
            "used": mem.used / 1024**3,  # GB
            "percent": mem.percent
        }
    except ImportError:
        print("è­¦å‘Š: æ— æ³•å¯¼å…¥psutilåº“ï¼Œç³»ç»Ÿå†…å­˜æ£€æµ‹åŠŸèƒ½å°†ä¸å¯ç”¨" + " | " + "Warning: Failed to import psutil library, system memory detection will be unavailable")
        return {
            "total": 0,
            "available": 0,
            "used": 0,
            "percent": 0
        }

def get_device_info():
    """è·å–è®¾å¤‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬GPUå’ŒCPUï¼Œå¹¶åˆ†ææœ€ä½³è¿è¡Œè®¾å¤‡"""
    """Get device information, including GPU and CPU, and analyze the optimal running device"""
    device_info = {
        "device_type": "unknown",
        "gpu": get_gpu_info(),
        "system_memory": get_system_memory_info(),
        "recommended_device": "cpu",  # é»˜è®¤æ¨èCPU | Default recommended device: CPU
        "memory_sufficient": True,
        "warning_message": None
    }
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºApple Silicon
    # Check if it's Apple Silicon
    try:
        import platform
        if platform.system() == "Darwin" and platform.processor() == "arm":
            device_info["device_type"] = "apple_silicon"
            # M1/M2èŠ¯ç‰‡æœ‰ç»Ÿä¸€å†…å­˜ï¼Œæ£€æŸ¥æ€»å†…å­˜æ˜¯å¦å……è¶³
            # M1/M2 chips have unified memory, check if total memory is sufficient
            if device_info["system_memory"]["total"] >= 16:  # è‡³å°‘16GBå†…å­˜ | At least 16GB of memory
                device_info["recommended_device"] = "mps"
            else:
                device_info["memory_sufficient"] = False
                device_info["warning_message"] = "Apple SiliconèŠ¯ç‰‡å†…å­˜ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨è‡³å°‘16GBå†…å­˜çš„è®¾å¤‡" + " | " + "Insufficient memory for Apple Silicon chip, it is recommended to use a device with at least 16GB of memory"
            return device_info
    except:
        pass
    
    # æ£€æŸ¥æ˜¯å¦æœ‰NVIDIA GPU
    # Check if there's an NVIDIA GPU
    if device_info["gpu"]["available"]:
        device_info["device_type"] = "nvidia_gpu"
        # æ£€æŸ¥GPUå†…å­˜æ˜¯å¦å……è¶³
        # Check if GPU memory is sufficient
        if device_info["gpu"]["total_memory"] >= 8:  # è‡³å°‘8GBæ˜¾å­˜ | At least 8GB of VRAM
            device_info["recommended_device"] = "cuda"
        else:
            # æ˜¾å­˜ä¸è¶³ï¼Œä½†ä»å¯ä½¿ç”¨ï¼Œåªæ˜¯æ€§èƒ½ä¼šå—å½±å“
            # Insufficient VRAM, but can still be used with performance impact
            device_info["memory_sufficient"] = False
            device_info["warning_message"] = "NVIDIA GPUæ˜¾å­˜ä¸è¶³ï¼Œå¯èƒ½ä¼šä½¿ç”¨ç³»ç»Ÿå†…å­˜ï¼Œæ€§èƒ½ä¼šä¸‹é™" + " | " + "Insufficient VRAM for NVIDIA GPU, system memory may be used, performance will degrade"
            device_info["recommended_device"] = "cuda"  # ä»æ¨èä½¿ç”¨GPUï¼Œä½†ä¼šå¯ç”¨å†…å­˜ä¼˜åŒ– | Still recommended to use GPU with memory optimization enabled
        return device_info
    
    # é»˜è®¤ä¸ºCPU
    # Default to CPU
    device_info["device_type"] = "cpu"
    # æ£€æŸ¥ç³»ç»Ÿå†…å­˜æ˜¯å¦å……è¶³
    # Check if system memory is sufficient
    if device_info["system_memory"]["total"] < 8:
        device_info["memory_sufficient"] = False
        device_info["warning_message"] = "ç³»ç»Ÿå†…å­˜ä¸è¶³ï¼Œæ¨¡å‹è¿è¡Œå¯èƒ½ä¼šéå¸¸ç¼“æ…¢" + " | " + "Insufficient system memory, model operation may be very slow"
    
    return device_info

def calculate_required_memory(model_name, quantization, use_cpu=False, use_mps=False):
    """æ ¹æ®æ¨¡å‹åç§°ã€é‡åŒ–æ–¹å¼å’Œè®¾å¤‡ç±»å‹è®¡ç®—æ‰€éœ€å†…å­˜"""
    """Calculate the required memory based on model name, quantization method, and device type"""
    model_info = MODEL_REGISTRY.get(model_name, {})
    vram_config = model_info.get("vram_requirement", {})
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»é‡åŒ–
    # Check if the model is already quantized
    is_quantized_model = model_info.get("quantized", False)
    
    # åŸºç¡€å†…å­˜éœ€æ±‚è®¡ç®—
    # Basic memory requirement calculation
    if is_quantized_model:
        base_memory = vram_config.get("full", 0)
    else:
        if quantization == "ğŸ‘ 4-bit (VRAM-friendly)":
            base_memory = vram_config.get("4bit", 0)
        elif quantization == "âš–ï¸ 8-bit (Balanced Precision)":
            base_memory = vram_config.get("8bit", 0)
        else:
            base_memory = vram_config.get("full", 0)
    
    # è°ƒæ•´å†…å­˜éœ€æ±‚ï¼ˆCPUå’ŒMPSé€šå¸¸éœ€è¦æ›´å¤šå†…å­˜ï¼‰
    # Adjust memory requirements (CPU and MPS usually require more memory)
    if use_cpu or use_mps:
        # CPUå’ŒMPSé€šå¸¸éœ€è¦æ›´å¤šå†…å­˜ç”¨äºå†…å­˜äº¤æ¢
        # CPU and MPS usually require more memory for memory swapping
        memory_factor = 1.5 if use_cpu else 1.2
        return base_memory * memory_factor
    
    return base_memory

def check_flash_attention():
    """æ£€æµ‹Flash Attention 2æ”¯æŒï¼ˆéœ€Ampereæ¶æ„åŠä»¥ä¸Šï¼‰"""
    """Check Flash Attention 2 support (requires Ampere architecture or higher)"""
    try:
        from flash_attn import flash_attn_func
        major, _ = torch.cuda.get_device_capability()
        return major >= 8  # ä»…æ”¯æŒè®¡ç®—èƒ½åŠ›8.0+çš„GPU | Only supports GPUs with compute capability 8.0+
    except ImportError:
        return False


FLASH_ATTENTION_AVAILABLE = check_flash_attention()


def init_qwen_paths(model_name):
    """åˆå§‹åŒ–æ¨¡å‹è·¯å¾„ï¼Œæ”¯æŒåŠ¨æ€ç”Ÿæˆä¸åŒæ¨¡å‹ç‰ˆæœ¬çš„è·¯å¾„"""
    """Initialize model paths, supporting dynamic generation of paths for different model versions"""
    base_dir = Path(folder_paths.models_dir).resolve()
    qwen_dir = base_dir / "Qwen" / "Qwen-VL"  # æ·»åŠ VLMå­ç›®å½• | Add VLM subdirectory
    model_dir = qwen_dir / model_name  # ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºå­ç›®å½• | Use model name as subdirectory
    
    # åˆ›å»ºç›®å½•
    # Create directories
    model_dir.mkdir(parents=True, exist_ok=True)
    
    # æ³¨å†Œåˆ°ComfyUI
    # Register to ComfyUI
    if hasattr(folder_paths, "add_model_folder_path"):
        folder_paths.add_model_folder_path("Qwen", str(model_dir))
    else:
        folder_paths.folder_names_and_paths["Qwen"] = ([str(model_dir)], {'.safetensors', '.bin'})
    
    print(f"æ¨¡å‹è·¯å¾„å·²åˆå§‹åŒ–: {model_dir}" + " | " + f"Model path initialized: {model_dir}")
    return str(model_dir)


def test_download_speed(url):
    """æµ‹è¯•ä¸‹è½½é€Ÿåº¦ï¼Œä¸‹è½½ 5 ç§’"""
    """Test download speed by downloading for 5 seconds"""
    try:
        start_time = time.time()
        response = requests.get(url, stream=True, timeout=10)
        downloaded_size = 0
        for data in response.iter_content(chunk_size=1024):
            if time.time() - start_time > 5:
                break
            downloaded_size += len(data)
        end_time = time.time()
        speed = downloaded_size / (end_time - start_time) / 1024  # KB/s
        return speed
    except Exception as e:
        print(f"æµ‹è¯•ä¸‹è½½é€Ÿåº¦æ—¶å‡ºç°é”™è¯¯: {e}" + " | " + f"Error testing download speed: {e}")
        return 0


def validate_model_path(model_path, model_name):
    """éªŒè¯æ¨¡å‹è·¯å¾„çš„æœ‰æ•ˆæ€§å’Œæ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨"""
    """Validate the effectiveness of the model path and check if model files are complete"""
    path_obj = Path(model_path)
    
    # åŸºæœ¬è·¯å¾„æ£€æŸ¥
    # Basic path check
    if not path_obj.is_absolute():
        print(f"é”™è¯¯: {model_path} ä¸æ˜¯ç»å¯¹è·¯å¾„" + " | " + f"Error: {model_path} is not an absolute path")
        return False
    
    if not path_obj.exists():
        print(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}" + " | " + f"Model directory does not exist: {model_path}")
        return False
    
    if not path_obj.is_dir():
        print(f"é”™è¯¯: {model_path} ä¸æ˜¯ç›®å½•" + " | " + f"Error: {model_path} is not a directory")
        return False
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦é½å…¨
    # Check if model files are complete
    if not check_model_files_exist(model_path, model_name):
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å®Œæ•´: {model_path}" + " | " + f"Model files are incomplete: {model_path}")
        return False
    
    return True


def check_model_files_exist(model_dir, model_name):
    """æ£€æŸ¥ç‰¹å®šæ¨¡å‹ç‰ˆæœ¬æ‰€éœ€çš„æ–‡ä»¶æ˜¯å¦é½å…¨"""
    """Check if all required files for a specific model version are present"""
    if model_name not in MODEL_REGISTRY:
        print(f"é”™è¯¯: æœªçŸ¥æ¨¡å‹ç‰ˆæœ¬ {model_name}" + " | " + f"Error: Unknown model version {model_name}")
        return False
    
    required_files = MODEL_REGISTRY[model_name]["required_files"]
    for file in required_files:
        if not os.path.exists(os.path.join(model_dir, file)):
            return False
    return True


# è§†é¢‘å¤„ç†å·¥å…·ç±»
# Video processing utility class
class VideoProcessor:
    def __init__(self):
        # å°è¯•å¯¼å…¥torchcodecä½œä¸ºé¦–é€‰è§†é¢‘å¤„ç†åº“
        # Try importing torchcodec as the preferred video processing library
        self.use_torchcodec = False
        self.use_opencv = False
        
        try:
            import torchcodec
            # æ£€æŸ¥VideoDecoderå±æ€§æ˜¯å¦å­˜åœ¨
            # Check if the VideoDecoder attribute exists
            if hasattr(torchcodec, 'VideoDecoder'):
                self.use_torchcodec = True
                print("ä½¿ç”¨torchcodecè¿›è¡Œè§†é¢‘å¤„ç†" + " | " + "Using torchcodec for video processing")
            else:
                print("torchcodecåº“ä¸­æ²¡æœ‰VideoDecoderå±æ€§" + " | " + "torchcodec library does not have the VideoDecoder attribute")
                raise ImportError
        except ImportError:
            print("torchcodecä¸å¯ç”¨" + " | " + "torchcodec is not available")
            if OPENCV_AVAILABLE:
                self.use_opencv = True
                print("ä½¿ç”¨OpenCVä½œä¸ºå¤‡é€‰è§†é¢‘å¤„ç†åº“" + " | " + "Using OpenCV as an alternative video processing library")
            else:
                print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„è§†é¢‘å¤„ç†åº“ï¼Œå°†å°è¯•ä½¿ç”¨torchvisionï¼ˆå¯èƒ½æœ‰å¼ƒç”¨è­¦å‘Šï¼‰" + " | " + "Warning: No available video processing library found, will attempt to use torchvision (may have deprecation warnings)")
                # æŠ‘åˆ¶torchvisionè§†é¢‘APIå¼ƒç”¨è­¦å‘Š
                # Suppress torchvision video API deprecation warnings
                import warnings
                warnings.filterwarnings("ignore", category=UserWarning, module="torchvision.io")
    
    def read_video(self, video_path):
        """è¯»å–è§†é¢‘æ–‡ä»¶å¹¶è¿”å›å¸§æ•°æ®"""
        """Read video file and return frame data"""
        start_time = time.time()
        try:
            if self.use_torchcodec:
                # ä½¿ç”¨torchcodecè¯»å–è§†é¢‘
                # Read video using torchcodec
                import torchcodec
                decoder = torchcodec.VideoDecoder(video_path)
                frames = []
                for frame in decoder:
                    frames.append(frame)
                fps = decoder.get_fps()
                total_frames = len(frames)
                frames = torch.stack(frames) if frames else torch.zeros(0)
                print(f"ä½¿ç”¨torchcodecæˆåŠŸå¤„ç†è§†é¢‘: {video_path}" + " | " + f"Successfully processed video with torchcodec: {video_path}")
            elif self.use_opencv:
                # ä½¿ç”¨OpenCVè¯»å–è§†é¢‘
                # Read video using OpenCV
                cap = cv2.VideoCapture(video_path)
                if not cap.isOpened():
                    raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}" + " | " + f"Failed to open video file: {video_path}")
                
                fps = cap.get(cv2.CAP_PROP_FPS)
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                frames = []
                
                while True:
                    ret, frame = cap.read()
                    if not ret:
                        break
                    # è½¬æ¢ä¸ºRGBå¹¶è½¬ä¸ºPyTorchå¼ é‡
                    # Convert to RGB and convert to PyTorch tensor
                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0
                    frames.append(frame)
                
                # ä¿®æ­£ï¼šä½¿ç”¨release()æ–¹æ³•é‡Šæ”¾èµ„æº
                # Fix: Use release() method to release resources
                cap.release()
                frames = torch.stack(frames) if frames else torch.zeros(0)
                print(f"ä½¿ç”¨OpenCVæˆåŠŸå¤„ç†è§†é¢‘: {video_path}" + " | " + f"Successfully processed video with OpenCV: {video_path}")
            else:
                # ä½¿ç”¨torchvisionè¯»å–è§†é¢‘ï¼ˆå¼ƒç”¨APIï¼‰
                # Read video using torchvision (deprecated API)
                frames, _, info = torchvision.io.read_video(video_path, pts_unit="sec")
                fps = info["video_fps"]
                total_frames = frames.shape[0]
                frames = frames.permute(0, 3, 1, 2).float() / 255.0  # è½¬æ¢ä¸º[B, C, H, W]æ ¼å¼ | Convert to [B, C, H, W] format
                print(f"ä½¿ç”¨torchvisionæˆåŠŸå¤„ç†è§†é¢‘: {video_path}" + " | " + f"Successfully processed video with torchvision: {video_path}")
            
            process_time = time.time() - start_time
            print(f"è§†é¢‘å¤„ç†å®Œæˆ: {video_path}, æ€»å¸§æ•°: {total_frames}, FPS: {fps:.2f}, å¤„ç†æ—¶é—´: {process_time:.3f}s" + " | " + f"Video processing completed: {video_path}, total frames: {total_frames}, FPS: {fps:.2f}, processing time: {process_time:.3f}s")
            return frames, fps, total_frames
            
        except Exception as e:
            print(f"è§†é¢‘å¤„ç†é”™è¯¯: {e}" + " | " + f"Video processing error: {e}")
            return None, None, None

class QwenVisionParser:
    def __init__(self):
        # é»˜è®¤ä½¿ç”¨æ³¨å†Œè¡¨ä¸­çš„ç¬¬ä¸€ä¸ªé»˜è®¤æ¨¡å‹
        # Use the first default model in the registry by default
        default_model = next((name for name, info in MODEL_REGISTRY.items() if info.get("default", False)), 
                            list(MODEL_REGISTRY.keys())[0])
        
        # é‡ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…å¹²æ‰°
        # Reset environment variables to avoid interference
        os.environ.pop("HUGGINGFACE_HUB_CACHE", None)     

        self.current_model_name = default_model
        self.current_quantization = None  # è®°å½•å½“å‰çš„é‡åŒ–é…ç½® | Record the current quantization configuration
        self.model_path = init_qwen_paths(self.current_model_name)
        self.cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        print(f"æ¨¡å‹è·¯å¾„: {self.model_path}" + " | " + f"Model path: {self.model_path}")
        print(f"ç¼“å­˜è·¯å¾„: {self.cache_dir}" + " | " + f"Cache path: {self.cache_dir}")
        
        # éªŒè¯å¹¶åˆ›å»ºç¼“å­˜ç›®å½•
        # Validate and create cache directory
        Path(self.cache_dir).mkdir(parents=True, exist_ok=True)

        self.model = None
        self.processor = None
        self.tokenizer = None
        self.video_processor = VideoProcessor()  # åˆå§‹åŒ–è§†é¢‘å¤„ç†å™¨ | Initialize video processor
        self.last_generated_text = ""  # ä¿å­˜ä¸Šæ¬¡ç”Ÿæˆçš„æ–‡æœ¬ï¼Œç”¨äºè°ƒè¯• | Save last generated text for debugging
        self.generation_stats = {"count": 0, "total_time": 0}  # ç»Ÿè®¡ç”Ÿæˆæ€§èƒ½ | Statistics for generation performance
        
        # åˆå§‹åŒ–è®¾å¤‡ä¿¡æ¯
        # Initialize device information
        self.device_info = get_device_info()
        self.default_device = self.device_info["recommended_device"]
        
        print(f"æ£€æµ‹åˆ°çš„è®¾å¤‡: {self.device_info['device_type']}" + " | " + f"Detected device: {self.device_info['device_type']}")
        print(f"è‡ªåŠ¨é€‰æ‹©çš„è¿è¡Œè®¾å¤‡: {self.default_device}" + " | " + f"Automatically selected running device: {self.default_device}")
        
        if not self.device_info["memory_sufficient"]:
            print(f"è­¦å‘Š: {self.device_info['warning_message']}" + " | " + f"Warning: {self.device_info['warning_message']}")
        
        # åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–é€‰é¡¹
        # Initialize memory optimization options
        self.optimize_for_low_memory = not self.device_info["memory_sufficient"]

    def clear_model_resources(self):
        """é‡Šæ”¾å½“å‰æ¨¡å‹å ç”¨çš„èµ„æº"""
        """Release resources occupied by the current model"""
        if self.model is not None:
            print("é‡Šæ”¾å½“å‰æ¨¡å‹å ç”¨çš„èµ„æº..." + " | " + "Releasing resources occupied by the current model...")
            del self.model, self.processor, self.tokenizer
            self.model = None
            self.processor = None
            self.tokenizer = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜ | Clean GPU cache

        # æ›´æ–°è®¾å¤‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œå› ä¸ºåˆå§‹åŒ–æ—¶å·²è®¾ç½®ï¼‰
        # Update device information (optional, already set during initialization)
        # self.device_info = get_device_info()
        # self.default_device = self.device_info["recommended_device"]
        
        # åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–é€‰é¡¹
        # Initialize memory optimization options
        self.optimize_for_low_memory = not self.device_info["memory_sufficient"]


    def check_memory_requirements(self, model_name, quantization):
        """æ£€æŸ¥å½“å‰è®¾å¤‡å†…å­˜æ˜¯å¦æ»¡è¶³æ¨¡å‹è¦æ±‚ï¼Œå¿…è¦æ—¶è°ƒæ•´é‡åŒ–çº§åˆ«"""
        """Check if the current device memory meets the model requirements, adjust quantization level if necessary"""
        # ä½¿ç”¨è‡ªåŠ¨é€‰æ‹©çš„è®¾å¤‡
        # Use the automatically selected device
        device = self.default_device
        use_cpu = device == "cpu"
        use_mps = device == "mps"
        
        # è®¡ç®—æ‰€éœ€å†…å­˜
        # Calculate required memory
        required_memory = calculate_required_memory(model_name, quantization, use_cpu, use_mps)
        
        if use_cpu or use_mps:
            # æ£€æŸ¥ç³»ç»Ÿå†…å­˜
            # Check system memory
            available_memory = self.device_info["system_memory"]["available"]
            memory_type = "ç³»ç»Ÿå†…å­˜" + " | " + "System memory"
        else:
            # æ£€æŸ¥GPUå†…å­˜
            # Check GPU memory
            available_memory = self.device_info["gpu"]["free_memory"]
            memory_type = "GPUæ˜¾å­˜" + " | " + "GPU VRAM"
        
        # æ·»åŠ 20%çš„å®‰å…¨ä½™é‡
        # Add a 20% safety margin
        safety_margin = 1.2
        required_memory_with_margin = required_memory * safety_margin
        
        print(f"æ¨¡å‹ {model_name} (é‡åŒ–: {quantization}) éœ€è¦ {required_memory:.2f} GB {memory_type}" + " | " + f"Model {model_name} (quantization: {quantization}) requires {required_memory:.2f} GB {memory_type}")
        print(f"è€ƒè™‘å®‰å…¨ä½™é‡åï¼Œéœ€è¦ {required_memory_with_margin:.2f} GB {memory_type}" + " | " + f"After considering safety margin, requires {required_memory_with_margin:.2f} GB {memory_type}")
        print(f"å½“å‰å¯ç”¨ {memory_type}: {available_memory:.2f} GB" + " | " + f"Currently available {memory_type}: {available_memory:.2f} GB")
        
        # å¦‚æœå†…å­˜ä¸è¶³ï¼Œè‡ªåŠ¨è°ƒæ•´é‡åŒ–çº§åˆ«
        # Automatically adjust quantization level if memory is insufficient
        if required_memory_with_margin > available_memory:
            print(f"è­¦å‘Š: æ‰€é€‰é‡åŒ–çº§åˆ«éœ€è¦çš„{memory_type}è¶…è¿‡å¯ç”¨å†…å­˜ï¼Œè‡ªåŠ¨è°ƒæ•´é‡åŒ–çº§åˆ«" + " | " + f"Warning: The selected quantization level requires more {memory_type} than available, automatically adjusting quantization level")
            
            # é™çº§ç­–ç•¥
            # Downgrade strategy
            if quantization == "ğŸš« None (Original Precision)":
                print("å°†é‡åŒ–çº§åˆ«ä»'æ— é‡åŒ–'è°ƒæ•´ä¸º'8-bit'" + " | " + "Adjusting quantization level from 'No quantization' to '8-bit'")
                return "âš–ï¸ 8-bit (Balanced Precision)"
            elif quantization == "âš–ï¸ 8-bit (Balanced Precision)":
                print("å°†é‡åŒ–çº§åˆ«ä»'8-bit'è°ƒæ•´ä¸º'4-bit'" + " | " + "Adjusting quantization level from '8-bit' to '4-bit'")
                return "ğŸ‘ 4-bit (VRAM-friendly)"
            else:
                # å·²ç»æ˜¯4-bitï¼Œæ— æ³•å†é™çº§
                # Already at 4-bit, cannot downgrade further
                print(f"é”™è¯¯: å³ä½¿ä½¿ç”¨4-bité‡åŒ–ï¼Œæ¨¡å‹ä»ç„¶éœ€è¦æ›´å¤š{memory_type}" + " | " + f"Error: Even with 4-bit quantization, the model still requires more {memory_type}")
                raise RuntimeError(f"é”™è¯¯: å¯ç”¨{memory_type}ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {required_memory_with_margin:.2f} GBï¼Œä½†åªæœ‰ {available_memory:.2f} GB" + " | " + f"Error: Insufficient available {memory_type}, requires at least {required_memory_with_margin:.2f} GB, but only {available_memory:.2f} GB available")
        
        return quantization

    
    def load_model(self, model_name, quantization):
        # æ£€æŸ¥å†…å­˜éœ€æ±‚å¹¶å¯èƒ½è°ƒæ•´é‡åŒ–çº§åˆ«
        # Check memory requirements and possibly adjust quantization level
        adjusted_quantization = self.check_memory_requirements(model_name, quantization)
        
        # ä½¿ç”¨è‡ªåŠ¨é€‰æ‹©çš„è®¾å¤‡
        # Use the automatically selected device
        device = self.default_device
        print(f"ä½¿ç”¨è®¾å¤‡: {device}" + " | " + f"Using device: {device}")

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½æ¨¡å‹
        # Check if the model needs to be reloaded
        if (self.model is not None and 
            self.current_model_name == model_name and 
            self.current_quantization == quantization):
            print(f"ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹: {model_name}ï¼Œé‡åŒ–: {quantization}" + " | " + f"Using already loaded model: {model_name}, quantization: {quantization}")
            return
        
        # éœ€è¦é‡æ–°åŠ è½½ï¼Œå…ˆé‡Šæ”¾ç°æœ‰èµ„æº
        # Need to reload, release existing resources first
        self.clear_model_resources()
        
        # æ›´æ–°å½“å‰æ¨¡å‹åç§°å’Œè·¯å¾„
        # Update current model name and path
        self.current_model_name = model_name
        self.model_path = init_qwen_paths(self.current_model_name)
        self.current_quantization = quantization

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”å®Œæ•´
        # Check if model files exist and are complete
        if not validate_model_path(self.model_path, self.current_model_name):
            print(f"æ£€æµ‹åˆ°æ¨¡å‹æ–‡ä»¶ç¼ºå¤±ï¼Œæ­£åœ¨ä¸ºä½ ä¸‹è½½ {model_name} æ¨¡å‹ï¼Œè¯·ç¨å€™..." + " | " + f"Model files detected as missing, downloading {model_name} model for you, please wait...")
            print(f"ä¸‹è½½å°†ä¿å­˜åœ¨: {self.model_path}" + " | " + f"Download will be saved to: {self.model_path}")
            
            # å¼€å§‹ä¸‹è½½é€»è¾‘
            # Start download logic
            try:
                # ä»æ³¨å†Œè¡¨è·å–æ¨¡å‹ä¿¡æ¯
                # Get model information from registry
                model_info = MODEL_REGISTRY[model_name]
                
                # æµ‹è¯•ä¸‹è½½é€Ÿåº¦
                # Test download speed
                huggingface_test_url = f"https://huggingface.co/{model_info['repo_id']['huggingface']}/resolve/main/{model_info['test_file']}"
                modelscope_test_url = f"https://modelscope.cn/api/v1/models/{model_info['repo_id']['modelscope']}/repo?Revision=master&FilePath={model_info['test_file']}"
                huggingface_speed = test_download_speed(huggingface_test_url)
                modelscope_speed = test_download_speed(modelscope_test_url)

                print(f"Hugging Faceä¸‹è½½é€Ÿåº¦: {huggingface_speed:.2f} KB/s" + " | " + f"Hugging Face download speed: {huggingface_speed:.2f} KB/s")
                print(f"ModelScopeä¸‹è½½é€Ÿåº¦: {modelscope_speed:.2f} KB/s" + " | " + f"ModelScope download speed: {modelscope_speed:.2f} KB/s")

                # æ ¹æ®ä¸‹è½½é€Ÿåº¦é€‰æ‹©ä¼˜å…ˆä¸‹è½½æº
                # Select preferred download source based on download speed
                if huggingface_speed > modelscope_speed * 1.5:
                    download_sources = [
                        (snapshot_download, model_info['repo_id']['huggingface'], "Hugging Face"),
                        (modelscope_snapshot_download, model_info['repo_id']['modelscope'], "ModelScope")
                    ]
                    print("åŸºäºä¸‹è½½é€Ÿåº¦åˆ†æï¼Œä¼˜å…ˆå°è¯•ä»Hugging Faceä¸‹è½½" + " | " + "Based on download speed analysis, attempting to download from Hugging Face first")
                else:
                    download_sources = [
                        (modelscope_snapshot_download, model_info['repo_id']['modelscope'], "ModelScope"),
                        (snapshot_download, model_info['repo_id']['huggingface'], "Hugging Face")
                    ]
                    print("åŸºäºä¸‹è½½é€Ÿåº¦åˆ†æï¼Œä¼˜å…ˆå°è¯•ä»ModelScopeä¸‹è½½" + " | " + "Based on download speed analysis, attempting to download from ModelScope first")

                max_retries = 3
                success = False
                final_error = None
                used_cache_path = None

                for download_func, repo_id, source in download_sources:
                    for retry in range(max_retries):
                        try:
                            print(f"å¼€å§‹ä» {source} ä¸‹è½½æ¨¡å‹ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰..." + " | " + f"Starting to download model from {source} (attempt {retry + 1})...")
                            if download_func == snapshot_download:
                                cached_path = download_func(
                                    repo_id,
                                    cache_dir=self.cache_dir,
                                    ignore_patterns=["*.msgpack", "*.h5"],
                                    resume_download=True,
                                    local_files_only=False
                                )
                            else:
                                cached_path = download_func(
                                    repo_id,
                                    cache_dir=self.cache_dir,
                                    revision="master"
                                )

                            used_cache_path = cached_path  # è®°å½•ä½¿ç”¨çš„ç¼“å­˜è·¯å¾„ | Record the cache path used
                            
                            # å°†ä¸‹è½½çš„æ¨¡å‹å¤åˆ¶åˆ°æ¨¡å‹ç›®å½•
                            # Copy the downloaded model to the model directory
                            self.copy_cached_model_to_local(cached_path, self.model_path)
                            
                            print(f"æˆåŠŸä» {source} ä¸‹è½½æ¨¡å‹åˆ° {self.model_path}" + " | " + f"Successfully downloaded model from {source} to {self.model_path}")
                            success = True
                            break

                        except Exception as e:
                            final_error = e  # ä¿å­˜æœ€åä¸€ä¸ªé”™è¯¯ | Save the last error
                            if retry < max_retries - 1:
                                print(f"ä» {source} ä¸‹è½½æ¨¡å‹å¤±è´¥ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰: {e}ï¼Œå³å°†è¿›è¡Œä¸‹ä¸€æ¬¡å°è¯•..." + " | " + f"Failed to download model from {source} (attempt {retry + 1}): {e}, proceeding to next attempt...")
                            else:
                                print(f"ä» {source} ä¸‹è½½æ¨¡å‹å¤±è´¥ï¼ˆç¬¬ {retry + 1} æ¬¡å°è¯•ï¼‰: {e}ï¼Œå°è¯•å…¶ä»–æº..." + " | " + f"Failed to download model from {source} (attempt {retry + 1}): {e}, trying other source...")
                    if success:
                        break
                else:
                    raise RuntimeError("ä»æ‰€æœ‰æºä¸‹è½½æ¨¡å‹å‡å¤±è´¥ã€‚" + " | " + "Failed to download model from all sources.")
                
                # ä¸‹è½½å®Œæˆåå†æ¬¡éªŒè¯
                # Verify again after download is complete
                if not validate_model_path(self.model_path, self.current_model_name):
                    raise RuntimeError(f"ä¸‹è½½åæ¨¡å‹æ–‡ä»¶ä»ä¸å®Œæ•´: {self.model_path}" + " | " + f"Model files still incomplete after download: {self.model_path}")
                
                print(f"æ¨¡å‹ {model_name} å·²å‡†å¤‡å°±ç»ª" + " | " + f"Model {model_name} is ready")
                
            except Exception as e:
                print(f"ä¸‹è½½æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}" + " | " + f"Error occurred while downloading model: {e}")
                
                # ä¸‹è½½å¤±è´¥æç¤º
                # Download failure prompt
                if used_cache_path:
                    print("\nâš ï¸ æ³¨æ„ï¼šä¸‹è½½è¿‡ç¨‹ä¸­åˆ›å»ºäº†ç¼“å­˜æ–‡ä»¶" + " | " + "\nâš ï¸ Note: Cache files were created during the download")
                    print(f"ç¼“å­˜è·¯å¾„: {used_cache_path}" + " | " + f"Cache path: {used_cache_path}")
                    print("ä½ å¯ä»¥å‰å¾€æ­¤è·¯å¾„åˆ é™¤ç¼“å­˜æ–‡ä»¶ä»¥é‡Šæ”¾ç¡¬ç›˜ç©ºé—´" + " | " + "You can go to this path to delete the cache files to free up disk space")
                
                raise RuntimeError(f"æ— æ³•ä¸‹è½½æ¨¡å‹ {model_name}ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®åˆ° {self.model_path}" + " | " + f"Failed to download model {model_name}, please download manually and place in {self.model_path}")

        # æ¨¡å‹æ–‡ä»¶å®Œæ•´ï¼Œæ­£å¸¸åŠ è½½
        # Model files are complete, load normally
        print(f"åŠ è½½æ¨¡å‹: {self.model_path}ï¼Œé‡åŒ–: {quantization}" + " | " + f"Loading model: {self.model_path}, quantization: {quantization}")

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»é‡åŒ–
        # Check if the model is already quantized
        is_quantized_model = MODEL_REGISTRY.get(model_name, {}).get("quantized", False)
        
        # é…ç½®é‡åŒ–å‚æ•°
        # Configure quantization parameters
        if is_quantized_model:
            print(f"æ¨¡å‹ {model_name} å·²ç»æ˜¯é‡åŒ–æ¨¡å‹ï¼Œå°†å¿½ç•¥ç”¨æˆ·çš„é‡åŒ–è®¾ç½®" + " | " + f"Model {model_name} is already a quantized model, user quantization settings will be ignored")
            # å¯¹äºå·²ç»é‡åŒ–çš„æ¨¡å‹ï¼Œä½¿ç”¨åŸå§‹ç²¾åº¦åŠ è½½
            # For already quantized models, load with original precision
            load_dtype = torch.float16
            quant_config = None
        else:
            # å¯¹äºéé‡åŒ–æ¨¡å‹ï¼Œåº”ç”¨ç”¨æˆ·é€‰æ‹©çš„é‡åŒ–è®¾ç½®
            # For non-quantized models, apply user-selected quantization settings
            if quantization == "ğŸ‘ 4-bit (VRAM-friendly)":
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True,
                )
                load_dtype = None  # è®©é‡åŒ–é…ç½®å†³å®šæ•°æ®ç±»å‹ | Let quantization configuration determine data type
            elif quantization == "âš–ï¸ 8-bit (Balanced Precision)":
                quant_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                )
                load_dtype = None  # è®©é‡åŒ–é…ç½®å†³å®šæ•°æ®ç±»å‹ | Let quantization configuration determine data type
            else:
                # ä¸ä½¿ç”¨é‡åŒ–ï¼Œä½¿ç”¨åŸå§‹ç²¾åº¦
                # No quantization, use original precision
                load_dtype = torch.float16
                quant_config = None

        # é…ç½®device_map
        # Configure device_map
        if device == "cuda":
            if torch.cuda.device_count() > 0:
                device_map = {"": 0}  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU | Use first GPU
                print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}" + " | " + f"Using GPU: {torch.cuda.get_device_name(0)}")
            else:
                device_map = "auto"
                print("æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå°†å°è¯•ä½¿ç”¨autoè®¾å¤‡æ˜ å°„" + " | " + "No available GPU detected, will attempt to use auto device mapping")
        elif device == "mps":
            device_map = "auto"  # MPSä¸æ”¯æŒdevice_mapï¼ŒåŠ è½½åéœ€æ‰‹åŠ¨ç§»åˆ°è®¾å¤‡ | MPS does not support device_map, need to manually move to device after loading
        else:
            device_map = "auto"  # CPUåŠ è½½ | CPU loading

        # å‡†å¤‡åŠ è½½å‚æ•°
        # Prepare loading parameters
        load_kwargs = {
            "device_map": device_map,
            "torch_dtype": load_dtype,
            "attn_implementation": "sdpa",
            "low_cpu_mem_usage": True,
            "use_safetensors": True,
        }

        # å¦‚æœæœ‰é‡åŒ–é…ç½®ï¼Œæ·»åŠ åˆ°åŠ è½½å‚æ•°ä¸­
        # If there's a quantization configuration, add it to the loading parameters
        if quant_config is not None:
            load_kwargs["quantization_config"] = quant_config

        # åŠ è½½æ¨¡å‹
        # Load model
        self.model = AutoModelForVision2Seq.from_pretrained(
            self.model_path,
            **load_kwargs
        ).eval()

        # å¯¹äºMPSï¼Œéœ€è¦æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
        # For MPS, need to manually move the model to the device
        if device == "mps":
            self.model = self.model.to("mps")

        # ç¼–è¯‘ä¼˜åŒ–
        # Compilation optimization
        if torch.__version__ >= "2.2":
            self.model = torch.compile(self.model, mode="reduce-overhead")

        # SDPä¼˜åŒ–
        # SDP optimization
        torch.backends.cuda.enable_flash_sdp(True)
        torch.backends.cuda.enable_mem_efficient_sdp(True)

        # åŠ è½½å¤„ç†å™¨å’Œåˆ†è¯å™¨
        # Load processor and tokenizer
        self.processor = AutoProcessor.from_pretrained(self.model_path, trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)

        # ä¿®å¤rope_scalingé…ç½®è­¦å‘Š
        # Fix rope_scaling configuration warning
        if hasattr(self.model.config, "rope_scaling"):
            del self.model.config.rope_scaling  # ç¦ç”¨ MROPE ä¼˜åŒ– | Disable MROPE optimization

    def copy_cached_model_to_local(self, cached_path, target_path):
        """å°†ç¼“å­˜çš„æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ°ç›®æ ‡è·¯å¾„"""
        """Copy cached model files to target path"""
        print(f"æ­£åœ¨å°†æ¨¡å‹ä»ç¼“å­˜å¤åˆ¶åˆ°: {target_path}" + " | " + f"Copying model from cache to: {target_path}")
        target_path = Path(target_path)
        target_path.mkdir(parents=True, exist_ok=True)
        
        # ä½¿ç”¨shutilè¿›è¡Œé€’å½’å¤åˆ¶
        # Use shutil for recursive copying
        import shutil
        for item in Path(cached_path).iterdir():
            if item.is_dir():
                shutil.copytree(item, target_path / item.name, dirs_exist_ok=True)
            else:
                shutil.copy2(item, target_path / item.name)
        
        # éªŒè¯å¤åˆ¶æ˜¯å¦æˆåŠŸ
        # Verify if the copy was successful
        if validate_model_path(target_path, self.current_model_name):
            print(f"æ¨¡å‹å·²æˆåŠŸå¤åˆ¶åˆ° {target_path}" + " | " + f"Model successfully copied to {target_path}")
        else:
            raise RuntimeError(f"å¤åˆ¶åæ¨¡å‹æ–‡ä»¶ä»ä¸å®Œæ•´: {target_path}" + " | " + f"Model files still incomplete after copy: {target_path}")

    def tensor_to_pil(self, image_tensor):
        """å°†å›¾åƒå¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ"""
        """Convert image tensor to PIL image"""
        if image_tensor.dim() == 4:
            image_tensor = image_tensor[0]
        image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
        return Image.fromarray(image_np)

    def preprocess_image(self, image):
        """é¢„å¤„ç†å›¾åƒï¼ŒåŒ…æ‹¬å°ºå¯¸è°ƒæ•´å’Œä¼˜åŒ–"""
        """Preprocess image, including resizing and optimization"""
        pil_image = self.tensor_to_pil(image)
        
        # é™åˆ¶æœ€å¤§å°ºå¯¸ï¼Œé¿å…è¿‡å¤§çš„è¾“å…¥
        # Limit maximum size to avoid excessively large inputs
        max_res = 1024
        if max(pil_image.size) > max_res:
            pil_image.thumbnail((max_res, max_res))
        
        # è½¬æ¢å›å¼ é‡å¹¶å½’ä¸€åŒ–
        # Convert back to tensor and normalize
        img_np = np.array(pil_image)
        img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0) / 255.0
        
        # è½¬å›PILå›¾åƒ
        # Convert back to PIL image
        pil_image = Image.fromarray((img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8))
        return pil_image

    def preprocess_video(self, video_path):
        """é¢„å¤„ç†è§†é¢‘ï¼ŒåŒ…æ‹¬å¸§æå–å’Œå°ºå¯¸è°ƒæ•´"""
        """Preprocess video, including frame extraction and resizing"""
        # ä½¿ç”¨è§†é¢‘å¤„ç†å™¨è¯»å–è§†é¢‘
        # Read video using video processor
        frames, fps, total_frames = self.video_processor.read_video(video_path)
        
        if frames is None:
            print(f"æ— æ³•å¤„ç†è§†é¢‘: {video_path}" + " | " + f"Failed to process video: {video_path}")
            return None, None, None
        
        # æ‰“å°åŸå§‹å¸§ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        # Print original frame information (for debugging)
        if frames.numel() > 0:
            print(f"åŸå§‹å¸§: å½¢çŠ¶={frames.shape}, ç±»å‹={frames.dtype}, æœ€å°å€¼={frames.min()}, æœ€å¤§å€¼={frames.max()}" + " | " + f"Original frames: shape={frames.shape}, type={frames.dtype}, min={frames.min()}, max={frames.max()}")
        
        # æ›´æ¿€è¿›çš„å¸§æ•°é‡é™åˆ¶
        # More aggressive frame count limit
        max_frames = 15
        if total_frames > max_frames:
            # é‡‡æ ·å¸§
            # Sample frames
            indices = np.linspace(0, total_frames - 1, max_frames, dtype=int)
            frames = frames[indices]
            print(f"è§†é¢‘å¸§æ•°é‡ä» {total_frames} é‡‡æ ·åˆ° {len(frames)}" + " | " + f"Video frames sampled from {total_frames} to {len(frames)}")
        
        # ç¡®ä¿å¸§æ•°æ®æ˜¯(C, H, W)æ ¼å¼ï¼Œå¹¶ä¸”æ˜¯float32ç±»å‹(0.0-1.0)
        # Ensure frame data is in (C, H, W) format and is float32 type (0.0-1.0)
        processed_frames = []
        for frame in frames:
            # ç¡®ä¿å¸§æ˜¯(C, H, W)æ ¼å¼
            # Ensure frame is in (C, H, W) format
            if frame.dim() == 3 and frame.shape[0] not in [1, 3]:
                # å¦‚æœç¬¬ä¸€ä¸ªç»´åº¦ä¸æ˜¯é€šé“æ•°(1æˆ–3)ï¼Œå¯èƒ½æ˜¯(H, W, C)æ ¼å¼
                # If the first dimension is not the number of channels (1 or 3), it might be in (H, W, C) format
                frame = frame.permute(2, 0, 1)
            
            # ç¡®ä¿å¸§æ˜¯float32ç±»å‹(0.0-1.0)
            # Ensure frame is float32 type (0.0-1.0)
            if frame.dtype != torch.float32:
                frame = frame.float()
            
            if frame.max() > 1.0:
                # å¦‚æœåƒç´ å€¼èŒƒå›´ä¸æ˜¯0.0-1.0ï¼Œè¿›è¡Œå½’ä¸€åŒ–
                # If pixel value range is not 0.0-1.0, normalize
                frame = frame / 255.0
            
            processed_frames.append(frame)
        
        # è°ƒæ•´å¸§å¤§å°
        # Resize frames
        resized_frames = []
        for frame in processed_frames:
            # è½¬æ¢ä¸ºPILå›¾åƒè¿›è¡Œè°ƒæ•´å¤§å°
            # Convert to PIL image for resizing
            # å…ˆè½¬æ¢ä¸º(H, W, C)æ ¼å¼ï¼Œå†è½¬æ¢ä¸ºnumpyæ•°ç»„å’Œuint8ç±»å‹
            # First convert to (H, W, C) format, then to numpy array and uint8 type
            frame_np = frame.permute(1, 2, 0).cpu().numpy()
            frame_np = (frame_np * 255).clip(0, 255).astype(np.uint8)
            frame_pil = Image.fromarray(frame_np)
            
            # è°ƒæ•´å¤§å°ä¸º384x384
            # Resize to 384x384
            frame_pil = frame_pil.resize((384, 384), Image.Resampling.LANCZOS)
            
            # è½¬å›å¼ é‡ (C, H, W) æ ¼å¼ï¼Œfloat32ç±»å‹(0.0-1.0)
            # Convert back to tensor (C, H, W) format, float32 type (0.0-1.0)
            frame_tensor = torch.from_numpy(np.array(frame_pil)).permute(2, 0, 1).float() / 255.0
            resized_frames.append(frame_tensor)
        
        # è½¬æ¢å›å¼ é‡
        # Convert back to tensor
        if resized_frames:
            resized_frames = torch.stack(resized_frames)
        else:
            resized_frames = torch.zeros(0)
        
        print(f"å¤„ç†åå¸§: å½¢çŠ¶={resized_frames.shape}, ç±»å‹={resized_frames.dtype}" + " | " + f"Processed frames: shape={resized_frames.shape}, type={resized_frames.dtype}")
        return resized_frames, fps, len(frames) # è¿”å›å®é™…é‡‡æ ·åçš„å¸§æ•° | Return the actual number of sampled frames
        

    @torch.no_grad()
    def process(
        self,
        model_name: str,
        quantization: str,
        prompt: str,
        temperature: float,
        top_p: float,
        top_k: int,
        num_beams: int,
        max_new_tokens: int,
        num_return_sequences: int,
        length_penalty: float,
        early_stopping: bool,
        constraints: str,
        repetition_penalty: float,
        no_repeat_ngram_size: int,
        min_length: int,
        bad_words_ids: str | None,
        typical_p: float,
        forced_bos_token_id: int,
        forced_eos_token_id: int,
        renormalize_logits: bool,
        do_sample: bool,
        seed: int,
        unload_after_generation: bool = True,
        image: torch.Tensor | None = None,
        video_path: str | None = None,
        **kwargs
    ):
        start_time = time.time()

        if seed == 0:
            seed = random.randint(0, 0xffffffffffffffff)
        
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        
        # ç¡®ä¿åŠ è½½æ­£ç¡®çš„æ¨¡å‹å’Œé‡åŒ–é…ç½®
        # Ensure correct model and quantization configuration are loaded
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½ä¸”æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½ï¼ˆå³ä½¿åç§°ç›¸åŒï¼‰
        # Check if model is already loaded and if it needs to be reloaded (even if the name is the same)
        if (self.model is not None and 
            self.current_model_name == model_name and 
            self.current_quantization == quantization):
            # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæ¨¡å‹æ˜¯é¢„é‡åŒ–çš„ï¼Œä½†ç”¨æˆ·é€‰æ‹©äº†é‡åŒ–é€‰é¡¹ï¼Œä»éœ€é‡æ–°åŠ è½½
            # Additional check: If the model is pre-quantized but the user selected a quantization option, still reload
            is_quantized_model = MODEL_REGISTRY.get(model_name, {}).get("quantized", False)
            user_selected_quantization = quantization in ["ğŸ‘ 4-bit (VRAM-friendly)", "âš–ï¸ 8-bit (Balanced Precision)"]
            
            if is_quantized_model and user_selected_quantization:
                print(f"æ¨¡å‹ {model_name} å·²ç»æ˜¯é‡åŒ–æ¨¡å‹ï¼Œå°†å¿½ç•¥ç”¨æˆ·çš„é‡åŒ–è®¾ç½®å¹¶é‡æ–°åŠ è½½" + " | " + f"Model {model_name} is already a quantized model, user quantization settings will be ignored and reloaded")
                self.clear_model_resources()
                self.load_model(model_name, "ğŸš« None (Original Precision)")
            else:
                print(f"ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹: {model_name}ï¼Œé‡åŒ–: {quantization}" + " | " + f"Using already loaded model: {model_name}, quantization: {quantization}")
        else:
            self.load_model(model_name, quantization)
        
        # å›¾åƒé¢„å¤„ç†
        # Image preprocessing
        pil_image = None
        if image is not None:
            pil_image = self.preprocess_image(image)
        
        # è§†é¢‘é¢„å¤„ç†
        # Video preprocessing
        video_frames = None
        if video_path:
            video_frames, video_fps, video_frames_count = self.preprocess_video(video_path)
            if video_frames is not None:
                print(f"è§†é¢‘å·²å¤„ç†: {video_path}, å¸§æ•°: {video_frames_count}, FPS: {video_fps}" + " | " + f"Video processed: {video_path}, frames: {video_frames_count}, FPS: {video_fps}")
        
        # æ„å»ºå¯¹è¯
        # Build conversation
        SYSTEM_PROMPT = "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving visual inputs and generating text."
        if model_name.startswith("Minthy"):
            SYSTEM_PROMPT = "You are image captioning expert, creative, unbiased and uncensored."
        print(f"SYSTEM_PROMPT = {SYSTEM_PROMPT}")
        
        conversation = [
            {"role": "system", "content": [{"type": "text", "text": SYSTEM_PROMPT}]},
            {"role": "user", "content": []}
        ]
        
        # æ·»åŠ å›¾åƒå’Œè§†é¢‘åˆ°å¯¹è¯
        # Add images and videos to the conversation
        if pil_image is not None:
            conversation[-1]["content"].append({"type": "image", "image": pil_image})
        
        if video_path and video_frames is not None:
            # è½¬æ¢è§†é¢‘å¸§ä¸ºPILå›¾åƒåˆ—è¡¨
            # Convert video frames to list of PIL images
            video_frame_list = []
            for frame in video_frames:
                frame = frame.permute(1, 2, 0).cpu().numpy() * 255
                frame = frame.astype(np.uint8)
                video_frame_list.append(Image.fromarray(frame))
            
            conversation[-1]["content"].append({"type": "video", "video": video_frame_list})
        
        # å¤„ç†ç”¨æˆ·æç¤º
        # Process user prompt
        user_prompt = prompt if prompt.endswith(("", ".", "ï¼", "ã€‚", "ï¼Ÿ", "ï¼")) else f"{prompt} "
        conversation[-1]["content"].append({"type": "text", "text": user_prompt})
        
        # åº”ç”¨èŠå¤©æ¨¡æ¿
        # Apply chat template
        input_text = self.processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)
        
        # å‡†å¤‡å¤„ç†å™¨å‚æ•°
        # Prepare processor parameters
        processor_args = {
            "text": input_text,
            "return_tensors": "pt",
            "padding": True,
        }
        
        # è°ƒç”¨å¤šæ¨¡æ€å¤„ç†é€»è¾‘
        # Call multimodal processing logic
        images, videos = process_vision_info(conversation)
        processor_args["images"] = images
        processor_args["videos"] = videos
        
        # æ¸…ç†ä¸å†éœ€è¦çš„å¤§å¯¹è±¡
        # Clean up large objects that are no longer needed
        del video_frames, images, videos
        torch.cuda.empty_cache()
        
        # å°†è¾“å…¥ç§»è‡³è®¾å¤‡
        # Move inputs to device
        inputs = self.processor(**processor_args)
        device = self.default_device
        model_inputs = {
            k: v.to(device)
            for k, v in inputs.items()
            if v is not None
        }
        
        # ç¡®ä¿model_inputsåŒ…å«æ‰€éœ€çš„é”®
        # Ensure model_inputs contains required keys
        if "input_ids" not in model_inputs:
            raise ValueError("å¤„ç†åçš„è¾“å…¥ä¸åŒ…å«'input_ids'é”®" + " | " + "Processed inputs do not contain 'input_ids' key")
        
        # BUGFIX: Process forced_eos_token_id into a list or None
        if forced_eos_token_id is not None:
            if forced_eos_token_id < 0:
                forced_eos_token_id_list = None
            else:
                forced_eos_token_id_list = [forced_eos_token_id]
        else:
            forced_eos_token_id_list = None

        # BUGFIX: Process bad_words_ids string into token IDs
        processed_bad_words_ids = None
        if bad_words_ids and self.tokenizer:
            bad_words_list = bad_words_ids.split("|")
            if bad_words_list:
                # Tokenize the words, ensuring not to add special tokens
                processed_bad_words_ids = self.tokenizer(bad_words_list, add_special_tokens=False).input_ids
        processed_constraints = None
        # ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒãŒæœ‰åŠ¹ãªå ´åˆï¼ˆnum_beams > 1ï¼‰ã«ã®ã¿åˆ¶ç´„ã‚’å‡¦ç†ã™ã‚‹
        if num_beams > 1 and constraints:
            constraint_phrases = constraints.split("|")
            try:
                # å„åˆ¶ç´„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€PhrasalConstraintã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                processed_constraints = [PhrasalConstraint(self.tokenizer(phrase, add_special_tokens=False).input_ids) for phrase in constraint_phrases if phrase.strip()]
            except Exception as e:
                print(f"åˆ¶ç´„ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}. åˆ¶ç´„ã‚’ç„¡è¦–ã—ã¾ã™ã€‚" + " | " + f"Error processing constraints: {e}. Ignoring constraints.")
                processed_constraints = None

        # ç”Ÿæˆé…ç½®
        # Generation configuration
        generate_config = {
            "max_new_tokens": max(max_new_tokens, 10),
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
            "num_beams": num_beams,
            "length_penalty": length_penalty,
            "early_stopping": early_stopping,
            # NOTE: 'constraints' expects a list of PhrasalConstraint objects, not just strings.
            # This implementation is basic and may not work as intended for complex constraints.
            "constraints": processed_constraints,
            "repetition_penalty": repetition_penalty,
            "eos_token_id": self.tokenizer.eos_token_id,
            "pad_token_id": self.tokenizer.pad_token_id,
            "use_cache": True,
            "bad_words_ids": processed_bad_words_ids, # BUGFIX: Use processed list of token IDs
            "no_repeat_ngram_size": no_repeat_ngram_size,
            "min_length": min_length,
            "output_scores": True,
            "return_dict_in_generate": True,
            "forced_bos_token_id": forced_bos_token_id,
            "forced_eos_token_id": forced_eos_token_id_list, # BUGFIX: Use the processed list
            "typical_p": typical_p,
            "renormalize_logits": renormalize_logits,
            "num_return_sequences": num_return_sequences,
            "do_sample": do_sample,
        }
        
        # å¦‚æœä½¿ç”¨æ³¢æŸæœç´¢ï¼Œåˆ™ç¦ç”¨é‡‡æ ·
        # If using beam search, disable sampling
        if num_beams > 1:
            generate_config["do_sample"] = False
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        # Record GPU memory usage
        if torch.cuda.is_available():
            pre_forward_memory = torch.cuda.memory_allocated() / 1024**2
            print(f"ç”Ÿæˆå‰GPUå†…å­˜ä½¿ç”¨: {pre_forward_memory:.2f} MB" + " | " + f"GPU memory usage before generation: {pre_forward_memory:.2f} MB")
        
        # ä½¿ç”¨é€‚å½“çš„è®¾å¤‡è¿›è¡Œç”Ÿæˆ
        # Generate using appropriate device
        with torch.no_grad():
            # ä½¿ç”¨æ–°çš„autocast API
            # Use new autocast API
            if device == "cuda":
                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):
                    outputs = self.model.generate(**model_inputs, **generate_config,)
            else:
                outputs = self.model.generate(**model_inputs, **generate_config,)
        
        # è®°å½•GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        # Record GPU memory usage
        if torch.cuda.is_available():
            post_forward_memory = torch.cuda.memory_allocated() / 1024**2
            print(f"ç”ŸæˆåGPUå†…å­˜ä½¿ç”¨: {post_forward_memory:.2f} MB" + " | " + f"GPU memory usage after generation: {post_forward_memory:.2f} MB")
            print(f"ç”Ÿæˆè¿‡ç¨‹ä¸­GPUå†…å­˜å¢åŠ : {post_forward_memory - pre_forward_memory:.2f} MB" + " | " + f"GPU memory increase during generation: {post_forward_memory - pre_forward_memory:.2f} MB")
        
        # Process outputs
        # When return_dict_in_generate=True, the output is an object. Access the token IDs via the .sequences attribute.
        text_tokens = outputs.sequences
        
        # Clean up large objects that are no longer needed
        del outputs, inputs
        torch.cuda.empty_cache()
        
        # Trim newly generated tokens
        input_length = model_inputs["input_ids"].shape[1]
        text_tokens = text_tokens[:, input_length:]  # æˆªå–æ–°ç”Ÿæˆçš„token | Trim newly generated tokens
        
        # è§£ç æ–‡æœ¬
        # Decode text
        text = self.tokenizer.decode(
            text_tokens[0],
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True
        )
        
        # ä¿å­˜ç”Ÿæˆçš„æ–‡æœ¬ç”¨äºè°ƒè¯•
        # Save generated text for debugging
        self.last_generated_text = text
        del model_inputs
        torch.cuda.empty_cache()
        
        # æ ¹æ®é€‰é¡¹å†³å®šæ˜¯å¦å¸è½½æ¨¡å‹
        # Decide whether to unload the model based on options
        if unload_after_generation:
            self.clear_model_resources()
            print(f"success_with_unload")
        else:
            print(f"success_keep_loaded")
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        # Calculate processing time
        process_time = time.time() - start_time
        self.generation_stats["count"] += 1
        self.generation_stats["total_time"] += process_time
        
        # æ‰“å°æ€§èƒ½ç»Ÿè®¡
        # Print performance statistics
        print(f"ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {process_time:.2f} ç§’" + " | " + f"Generation completed, time taken: {process_time:.2f} seconds")
        if self.generation_stats["count"] > 0:
            avg_time = self.generation_stats["total_time"] / self.generation_stats["count"]
            print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f} ç§’/æ¬¡" + " | " + f"Average generation time: {avg_time:.2f} seconds/time")
        
        return (text.strip(),)

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_name": (
                    list(MODEL_REGISTRY.keys()),
                    {
                        "default": next((name for name, info in MODEL_REGISTRY.items() if info.get("default", False)), 
                                       list(MODEL_REGISTRY.keys())[0]),
                        "tooltip": "Select the available model version."
                    }
                ),
                "quantization": (
                    [
                        "ğŸ‘ 4-bit (VRAM-friendly)",
                        "âš–ï¸ 8-bit (Balanced Precision)",
                        "ğŸš« None (Original Precision)"
                    ],
                    {
                        "default": "ğŸ‘ 4-bit (VRAM-friendly)",
                        "tooltip": "Select the quantization level:\nâœ… 4-bit: Significantly reduces VRAM usage, suitable for resource-constrained environments.\nâš–ï¸ 8-bit: Strikes a balance between precision and performance.\nğŸš« None: Uses the original floating-point precision (requires a high-end GPU)." + " | " + "é€‰æ‹©é‡åŒ–çº§åˆ«:\nâœ… 4-bit: æ˜¾è‘—å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼Œé€‚åˆèµ„æºå—é™ç¯å¢ƒã€‚\nâš–ï¸ 8-bit: å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½ã€‚\nğŸš« None: ä½¿ç”¨åŸå§‹æµ®ç‚¹ç²¾åº¦ï¼ˆéœ€è¦é«˜ç«¯GPUï¼‰ã€‚"
                    }
                ),
                "prompt": (
                    "STRING",
                    {
                        "default": "Describe this image in detail.",
                        "multiline": True,
                        "tooltip": "è‹±èªã¨çµµæ–‡å­—ã«å¯¾å¿œã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ä¾‹ï¼šã€Œç”»å®¶ã®ã‚¹ã‚¿ã‚¤ãƒ«ã§çŒ«ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ã€"
                    }
                ),
                "temperature": (
                    "FLOAT",
                    {
                        "default": 0.4, "min": 0.0, "max": 2.0, "step": 0.01,
                        "tooltip": "ä¸–ä»£ã®å¤šæ§˜æ€§ã‚’åˆ¶å¾¡ã—ã¾ã™:\nâ–«ï¸ ä½ã„ (ä¾‹: 0.2): ã‚ˆã‚Šæ±ºå®šè«–çš„ã‹ã¤é›†ä¸­çš„ã€‚\nâ–«ï¸ é«˜ã„ (ä¾‹: 0.8): ã‚ˆã‚Šãƒ©ãƒ³ãƒ€ãƒ ã‹ã¤å‰µé€ çš„ã€‚\n num_beamsãŒ2ä»¥ä¸Šã§ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚"
                    }
                ),
                "top_p": (
                    "FLOAT",
                    {
                        "default": 0.9, "min": 0.0, "max": 1.0, "step": 0.01,
                        "tooltip": "æ ¸ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠã«ãŠã‘ã‚‹ç´¯ç©ç¢ºç‡ã®ã‚«ãƒƒãƒˆã‚ªãƒ•ã€‚å€¤ãŒå°ã•ã„ã»ã©ã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚ˆã‚Šä¿å®ˆçš„ã«ãªã‚Šã¾ã™ã€‚\n num_beamsãŒ2ä»¥ä¸Šã§ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚"
                    }
                ),
                "top_k": (
                    "INT",
                    {
                        "default": 50, "min": 0, "max": 100, "step": 1,
                        "tooltip": "Top-Kã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚ç”Ÿæˆæ™‚ã«ä¿æŒã™ã‚‹ã€æœ€ã‚‚ç¢ºç‡ã®é«˜ã„èªå½™ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ã€‚ç„¡åŠ¹ã«ã™ã‚‹å ´åˆã¯0ã€‚\n num_beamsãŒ2ä»¥ä¸Šã§ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚"
                    }
                ),
                "num_beams": (
                    "INT",
                    {
                        "default": 1, "min": 1, "max": 16, "step": 1,
                        "tooltip": "ãƒ“ãƒ¼ãƒ æ¤œç´¢ã®ãƒ“ãƒ¼ãƒ æ•°ã€‚1 ã¯ãƒ“ãƒ¼ãƒ æ¤œç´¢ãŒãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚"
                    }
                
                ),
                "max_new_tokens": (
                    "INT",
                    {
                        "default": 40, "min": -1, "max": 999, "step": 1,
                        "tooltip": "ç”Ÿæˆã™ã‚‹æœ€å¤§ã®ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå˜èªã‚„æ–‡å­—ã®ã‹ãŸã¾ã‚Šï¼‰æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚"
                    }
                
                ),
                "num_return_sequences": (
                    "INT",
                    {
                        "default": 1, "min": 1, "max": 16, "step": 1,
                        "tooltip": "ç”Ÿæˆã™ã‚‹å€™è£œæ–‡ã®æ•°ã€‚num_beamsä»¥ä¸‹ã®å€¤ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€num_beams=5ã€num_return_sequences=3ã¨ã™ã‚‹ã¨ã€5ã¤ã®å€™è£œï¼ˆãƒ“ãƒ¼ãƒ ï¼‰ã‚’æ¢ç´¢ã—ã€ãã®ä¸­ã‹ã‚‰æœ€ã‚‚ã‚¹ã‚³ã‚¢ã®é«˜ã„3ã¤ã®æ–‡ç« ã‚’è¿”ã—ã¾ã™ã€‚"
                    }
                
                ),
                "length_penalty": (
                    "FLOAT",
                    {
                        "default": 1.0, "min": 0.0, "max": 5.0, "step": 0.1,
                        "tooltip": "ç”Ÿæˆã•ã‚Œã‚‹æ–‡ç« ã®é•·ã•ã‚’èª¿æ•´ã—ã¾ã™ã€‚1.0ãŒä¸­ç«‹ã§ã™ã€‚1.0ã‚ˆã‚Šå¤§ãã„å€¤ (ä¾‹: 1.2) ã‚’è¨­å®šã™ã‚‹ã¨é•·ã„æ–‡ç« ãŒç”Ÿæˆã•ã‚Œã‚„ã™ããªã‚Šã€1.0ã‚ˆã‚Šå°ã•ã„å€¤ (ä¾‹: 0.8) ã‚’è¨­å®šã™ã‚‹ã¨çŸ­ã„æ–‡ç« ãŒç”Ÿæˆã•ã‚Œã‚„ã™ããªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒçŸ­ã„æ–‡ã‚’å¥½ã‚€å‚¾å‘ã‚’è£œæ­£ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚"
                    }
                
                ),
                "early_stopping": (
                    "BOOLEAN",
                    {
                        "default": False,
                        "tooltip": "Trueã«è¨­å®šã™ã‚‹ã¨ã€å…¨ã¦ã®ãƒ“ãƒ¼ãƒ ãŒEOSãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ–‡ã®çµ‚ã‚ã‚Šã‚’ç¤ºã™ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«åˆ°é”ã—ãŸæ™‚ç‚¹ã§ç”Ÿæˆã‚’æ‰“ã¡åˆ‡ã‚Šã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä¸è¦ãªè¨ˆç®—ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚"
                    }
                ),
                "constraints": (
                    "STRING",
                    {
                        "default": "",
                        "tooltip": "ç”Ÿæˆã•ã‚Œã‚‹æ–‡ç« ã«å«ã‚ã‚‹ã¹ãå˜èªã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ã®æ©Ÿèƒ½ã§ã™ã€‚ã‚ˆã‚Šé«˜åº¦ãªåˆ¶å¾¡ãŒå¿…è¦ãªå ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚|ã‚’ä½¿ã£ã¦åŒºåˆ‡ã‚Šã¾ã™ã€‚"
                    }
                ),
                "repetition_penalty": (
                    "FLOAT",
                    {
                        "default": 1.0, "min": 0.0, "max": 2.0, "step": 0.01,
                        "tooltip": "ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¹°ã‚Šè¿”ã—ã«å¯¾ã™ã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£ã€‚1.0 ã¯ãƒšãƒŠãƒ«ãƒ†ã‚£ãŒãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚> 1.0 ã¯ç¹°ã‚Šè¿”ã—ã‚’æŠ‘åˆ¶ã—ã¾ã™ã€‚"
                    }
                ),
                "do_sample": (
                    "BOOLEAN",
                    {
                        "default": True,
                        "tooltip": "ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹ã«ã—ã¾ã™ã€‚\nâ–«ï¸ True: `temperature`, `top_p`, `top_k` ã‚’ä½¿ç”¨ã€‚\nâ–«ï¸ False: ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒãªã©æ±ºå®šè«–çš„ç”Ÿæˆã‚’è¡Œã„ã¾ã™ã€‚\n`num_beams=1` ã®å ´åˆã¯ True ã«ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚"
                    }
                ),
                "no_repeat_ngram_size": (
                    "INT",
                    {
                        "default": 0, "min": 0, "max": 10, "step": 1,
                        "tooltip": "åŒã˜ n-gramï¼ˆé€£ç¶šã—ãŸå˜èªåˆ—ï¼‰ã®ç¹°ã‚Šè¿”ã—ã‚’é˜²ãã¾ã™ã€‚0ã§ç„¡åŠ¹ã€‚\nä¾‹: 3 â†’ â€œthe cat in theâ€ ãŒ2å›å‡ºãªã„ã‚ˆã†ã«åˆ¶é™ã—ã¾ã™ã€‚"
                    }
                ),
                "min_length": (
                    "INT",
                    {
                        "default": 0, "min": 0, "max": 2048, "step": 1,
                        "tooltip": "ç”Ÿæˆã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚çŸ­ã™ãã‚‹å‡ºåŠ›ã‚’é¿ã‘ãŸã„å ´åˆã«è¨­å®šã—ã¾ã™ã€‚"
                    }
                ),
                "bad_words_ids": (
                    "STRING",
                    {
                        "default": "",
                        "tooltip": "å‡ºåŠ›ã«å«ã‚ãŸããªã„å˜èªã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ `|` åŒºåˆ‡ã‚Šã§å…¥åŠ›ã—ã¾ã™ã€‚æŒ‡å®šã•ã‚ŒãŸå˜èªã¯å¼·åˆ¶çš„ã«ç”Ÿæˆã‹ã‚‰é™¤å¤–ã•ã‚Œã¾ã™ã€‚ä¾‹: badword1|badword2"
                    }
                ),
                "typical_p": (
                    "FLOAT",
                    {
                        "default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01,
                        "tooltip": "å…¸å‹æ€§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã€‚top-pã¨ã¯ç•°ãªã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã€ã‚ˆã‚Šäººé–“ã‚‰ã—ã„å‡ºåŠ›ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\nå€¤ãŒå°ã•ã„ã»ã©ä¿å®ˆçš„ã«ãªã‚Šã¾ã™ã€‚"
                    }
                ),
                "forced_bos_token_id": (
                    "INT",
                    {
                        "default": -1,
                        "tooltip": "å‡ºåŠ›æ–‡ã®æœ€åˆã«å¼·åˆ¶çš„ã«ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ ID ã‚’æŒ‡å®šã—ã¾ã™ã€‚-1 ã§ç„¡åŠ¹ã€‚å¤šè¨€èªãƒ¢ãƒ‡ãƒ«ã‚„æŒ‡ç¤ºæ–‡ãªã©ã®ç”Ÿæˆåˆ¶å¾¡ã«ä½¿ã„ã¾ã™ã€‚"
                    }
                ),
                "forced_eos_token_id": (
                    "INT",
                    {
                        "default": -1,
                        "tooltip": "å‡ºåŠ›æ–‡ã®æœ€å¾Œã«å¼·åˆ¶çš„ã«ä½¿ç”¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ ID ã‚’æŒ‡å®šã—ã¾ã™ã€‚-1 ã§ç„¡åŠ¹ã€‚æ–‡æœ«å›ºå®šãªã©ã«ä½¿ã‚ã‚Œã¾ã™ã€‚"
                    }
                ),
                "renormalize_logits": (
                    "BOOLEAN",
                    {
                        "default": False,
                        "tooltip": "top-p ã‚„ top-k ã®å¾Œã« softmax ã‚’å†æ­£è¦åŒ–ã—ã¾ã™ã€‚\nã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šç†è«–çš„ã«æ•´ã£ãŸç¢ºç‡åˆ†å¸ƒã§ã®é¸æŠãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"
                    }
                ),
                "seed": ("INT", {
                    "default": 0, "min": 0, "max": 0xffffffffffffffff,
                    "tooltip": "ä¹±æ•°ç”Ÿæˆã®ã‚·ãƒ¼ãƒ‰ã€‚0ã®å ´åˆã€ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚"
                }),
                "unload_after_generation": (
                    "BOOLEAN",
                    {
                        "default": True,
                        "tooltip": "å‡¦ç†å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•çš„ã«ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾ã—ã¾ã™ã€‚è¤‡æ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€£ç¶šã—ã¦å‡¦ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã€ã“ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç„¡åŠ¹ã«ã™ã‚‹ã“ã¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"
                    }
                )
            },
            "optional": {
                "image": (
                    "IMAGE",
                    {
                        "tooltip": "å‚ç…§ç”»åƒï¼ˆPNG/JPG ã‚’ã‚µãƒãƒ¼ãƒˆï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯ç”»åƒã®å†…å®¹ã«åŸºã¥ã„ã¦ç”Ÿæˆçµæœã‚’èª¿æ•´ã—ã¾ã™ã€‚"
                    }
                ),
                "video_path": (
                    "VIDEO_PATH",
                    {
                        "tooltip": "ãƒ“ãƒ‡ã‚ª ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (MP4/WEBM ã‚’ã‚µãƒãƒ¼ãƒˆ) ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è¦–è¦šçš„ãªç‰¹å¾´ã‚’æŠ½å‡ºã—ã€ç”Ÿæˆã‚’æ”¯æ´ã—ã¾ã™ã€‚"
                    }
                )
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "process"
    CATEGORY = "ğŸ¼QwenVL"


class QwenVLTextParser:
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {"text": ("STRING", {}), "image": ("IMAGE", {})}}
    RETURN_TYPES = ("MASK", "IMAGE", "STRING")
    RETURN_NAMES = ("MASKS", "CROPPED_IMAGES", "caption")
    FUNCTION = "parse_text"
    CATEGORY = "ğŸ¼QwenVL"
    OUTPUT_IS_LIST = (False, True, False)

    def parse_text(self, text: str, image: torch.Tensor):
        # ComfyUIã®IMAGEã¯é€šå¸¸ (B, H, W, C) ãªã®ã§ã€å‡¦ç†ã—ã‚„ã™ã„ (B, C, H, W) ã«å¤‰æ›
        image_bchw = image.permute(0, 3, 1, 2)
        
        p_num = r"\d+"
        p_sep = r"\s*,?\s*"
        single_box_pattern = f"[\\[(]\\s*{p_num}{p_sep}{p_num}{p_sep}{p_num}{p_sep}{p_num}\\s*[\\])]"
        
        all_boxes = []
        seen_boxes = set()
        matches = re.finditer(single_box_pattern, text)
        for match in matches:
            box_string = match.group(0)
            try:
                numbers = [int(n) for n in re.findall(r'\d+', box_string)]
                if len(numbers) == 4:
                    box_tuple = tuple(numbers)
                    if box_tuple not in seen_boxes:
                        all_boxes.append(numbers)
                        seen_boxes.add(box_tuple)
            except (ValueError, TypeError):
                continue
        
        caption = re.sub(single_box_pattern, '', text).strip()
        caption = ' '.join(caption.split())

        # b, h, w, c ã‚’å–å¾—
        b, c, height, width = image_bchw.shape

        if not all_boxes:
            placeholder_mask = torch.zeros((1, height, width), dtype=torch.float32, device=image.device)
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚‚ (B, H, W, C) å½¢å¼ã§è¿”ã™
            placeholder_crop = torch.zeros((1, 64, 64, c), dtype=image.dtype, device=image.device)
            return (placeholder_mask, [placeholder_crop], caption)

        masks, crops = [], []
        
        MIN_SIZE = 4 

        for x1, y1, x2, y2 in all_boxes:
            x1, y1 = max(0, x1), max(0, y1)
            x2, y2 = min(width, x2), min(height, y2)
            
            if (x2 - x1) < MIN_SIZE or (y2 - y1) < MIN_SIZE:
                continue

            mask = torch.zeros((height, width), dtype=torch.float32, device=image.device)
            mask[y1:y2, x1:x2] = 1.0
            masks.append(mask)

            # (B, C, H, W) å½¢å¼ã®ãƒ†ãƒ³ã‚½ãƒ«ã‹ã‚‰ã‚¯ãƒ­ãƒƒãƒ—
            crop_bchw = image_bchw[0, :, y1:y2, x1:x2]
            # (C, H, W) -> (1, C, H, W) -> (1, H, W, C) ã«å¤‰æ›ã—ã¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
            crop_bhwc = crop_bchw.unsqueeze(0).permute(0, 2, 3, 1)
            crops.append(crop_bhwc)

        if not masks:
            placeholder_mask = torch.zeros((1, height, width), dtype=torch.float32, device=image.device)
            # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚‚ (B, H, W, C) å½¢å¼ã§è¿”ã™
            placeholder_crop = torch.zeros((1, 64, 64, c), dtype=image.dtype, device=image.device)
            return (placeholder_mask, [placeholder_crop], caption)
        
        return (torch.stack(masks), crops, caption)

NODE_CLASS_MAPPINGS = {
    "QwenVisionParser": QwenVisionParser,
    "QwenVLTextParser": QwenVLTextParser
}
NODE_DISPLAY_NAME_MAPPINGS = {
    "QwenVisionParser": "Qwen VL ğŸ¼",
    "QwenVLTextParser": "Qwen VL Text Parser ğŸ¼"
}